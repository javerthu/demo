RAID5至少三块磁盘组成，有一块校验盘。
在本次实验中，用四块，其中用一块磁盘做热备盘。
1、将四块磁盘分区，一个分区容量全给。。！！！！最后记得改分区类型为RAID类型
Command (m for help): n
Partition type:
   p   primary (0 primary, 0 extended, 4 free)
   e   extended
Select (default p): p
Partition number (1-4, default 1): 
First sector (2048-461373439, default 2048): 
Using default value 2048
Last sector, +sectors or +size{K,M,G} (2048-461373439, default 461373439): 
Using default value 461373439
Partition 1 of type Linux and of size 220 GiB is set

Command (m for help): t
Selected partition 1
Hex code (type L to list all codes): fd
Changed type of partition 'Linux' to 'Linux raid autodetect'         #########这里

Command (m for help): p

   Device Boot      Start         End      Blocks   Id  System
/dev/sdd1            2048   461373439   230685696   fd  Linux raid autodetect

Command (m for help): w
The partition table has been altered!

第二块一样的操作

[root@centos7 ~]# partprobe /dev/sd[b-e]

2、检查磁盘（保存一个良好的习惯）
[root@centos7 ~]# mdadm --examine /dev/sd[b-e]1
mdadm: No md superblock detected on /dev/sdb1.
mdadm: No md superblock detected on /dev/sdc1.
mdadm: No md superblock detected on /dev/sdd1.
mdadm: No md superblock detected on /dev/sde1.


3、创建RAID5组
[root@centos7 ~]# mdadm -C /dev/md5 -l 5 -n 3 -x 1 /dev/sd[b-e]1         
##### -C RAID路径  -l RAID组等级  -n  多少块活动磁盘    -x   多少块热备盘     /dev/sd[b-e]1(由哪些分区或者磁盘组成) 
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md5 started.    #######出现该信息成功


4、查看RAID组状态
[root@centos7 ~]# mdadm --detail /dev/md5 
/dev/md5:
           Version : 1.2
     Creation Time : Sun Mar 15 07:09:26 2020
        Raid Level : raid5                                                                 #####raid等级
        Array Size : 377221120 (359.75 GiB 386.27 GB)
     Used Dev Size : 188610560 (179.87 GiB 193.14 GB)
      Raid Devices : 3                                                                      ###活动盘
     Total Devices : 4                                                                      ####总盘
       Persistence : Superblock is persistent

     Intent Bitmap : Internal

       Update Time : Sun Mar 15 07:12:03 2020
             State : clean, degraded, recovering 
    Active Devices : 2
   Working Devices : 4
    Failed Devices : 0
     Spare Devices : 2

            Layout : left-symmetric
        Chunk Size : 512K

Consistency Policy : bitmap

    Rebuild Status : 16% complete                                                 ###第一次做坏盘修复崩了，感觉是这里的问题

              Name : centos7:5  (local to host centos7)
              UUID : 38d5b8cf:6594c83a:b53ba5a4:2ecaffc3
            Events : 32

    Number   Major   Minor   RaidDevice State
       0       8       17        0      active sync   /dev/sdb1
       1       8       33        1      active sync   /dev/sdc1
       4       8       49        2      spare rebuilding   /dev/sdd1        #####果然，MD这个盘还在重构，我把sdb1弄坏等于坏了两块盘，RAID组直接崩了

       3       8       65        -      spare   /dev/sde1                      ###########热备盘


############补充对的情况，千万别再有spare rebuilding盘的时候再去搞坏一个..
[root@centos7 ~]# mdadm --detail /dev/md5
/dev/md5:
           Version : 1.2
     Creation Time : Sun Mar 15 07:59:05 2020
        Raid Level : raid5
        Array Size : 377221120 (359.75 GiB 386.27 GB)
     Used Dev Size : 188610560 (179.87 GiB 193.14 GB)
      Raid Devices : 3
     Total Devices : 4
       Persistence : Superblock is persistent

     Intent Bitmap : Internal

       Update Time : Sun Mar 15 08:14:54 2020
             State : clean 
    Active Devices : 3
   Working Devices : 4
    Failed Devices : 0
     Spare Devices : 1

            Layout : left-symmetric
        Chunk Size : 512K

Consistency Policy : bitmap

              Name : centos7:5  (local to host centos7)
              UUID : 73e19978:73ff3239:db9abf74:c15a8f0e
            Events : 191

    Number   Major   Minor   RaidDevice State
       0       8       17        0      active sync   /dev/sdb1
       1       8       33        1      active sync   /dev/sdc1
       4       8       49        2      active sync   /dev/sdd1

       3       8       65        -      spare   /dev/sde1


5、创建RAID组文件系统
[root@centos7 ~]# mkfs -t ext4 /dev/md5 
mke2fs 1.42.9 (28-Dec-2013)
Filesystem label=
OS type: Linux
Block size=4096 (log=2)
Fragment size=4096 (log=2)
Stride=128 blocks, Stripe width=256 blocks
23576576 inodes, 94305280 blocks
4715264 blocks (5.00%) reserved for the super user
First data block=0
Maximum filesystem blocks=2241855488
2878 block groups
32768 blocks per group, 32768 fragments per group
8192 inodes per group
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done  



6、创建挂载目录并挂载
[root@centos7 ~]# mkdir /mnt/raid5
[root@centos7 ~]# mount /dev/md5 /mnt/raid5/
[root@centos7 ~]# mount -a
[root@centos7 ~]# df -hT
Filesystem     Type      Size  Used Avail Use% Mounted on
devtmpfs       devtmpfs  895M     0  895M   0% /dev
tmpfs          tmpfs     910M     0  910M   0% /dev/shm
tmpfs          tmpfs     910M   11M  900M   2% /run
tmpfs          tmpfs     910M     0  910M   0% /sys/fs/cgroup
/dev/sda3      xfs       195G  5.1G  190G   3% /
/dev/sda1      xfs      1014M  228M  787M  23% /boot
tmpfs          tmpfs     182M  4.0K  182M   1% /run/user/42
tmpfs          tmpfs     182M   24K  182M   1% /run/user/0
/dev/sr0       iso9660   4.4G  4.4G     0 100% /run/media/root/CentOS 7 x86_64
/dev/md5       ext4      354G   69M  336G   1% /mnt/raid5


7、！！保存RAID配置 ！重要！
[root@centos7 ~]# mdadm --detail --scan  --verbose  >>  /etc/mdadm.conf
[root@centos7 ~]# cat /etc/mdadm.conf 
ARRAY /dev/md5 level=raid5 num-devices=3 metadata=1.2 spares=2 name=centos7:5 UUID=38d5b8cf:6594c83a:b53ba5a4:2ecaffc3
   devices=/dev/sdb1,/dev/sdc1,/dev/sdd1,/dev/sde1

本来就可以使用了，但是我们模拟一下故障修复

8、模拟故障
[root@centos7 ~]# mdadm /dev/md5 --fail /dev/sdb1          ########弄坏sdb1分区
mdadm: set /dev/sdb1 faulty in /dev/md5

#在查看下md5状态
[root@centos7 ~]# mdadm --detail /dev/md5 
/dev/md5:
           Version : 1.2
     Creation Time : Sun Mar 15 07:09:26 2020
        Raid Level : raid5
        Array Size : 377221120 (359.75 GiB 386.27 GB)
     Used Dev Size : 188610560 (179.87 GiB 193.14 GB)
      Raid Devices : 3
     Total Devices : 4
       Persistence : Superblock is persistent

     Intent Bitmap : Internal

       Update Time : Sun Mar 15 07:26:20 2020
             State : clean, FAILED                                    ###############这是第一次操作死了的结果
    Active Devices : 1
   Working Devices : 3
    Failed Devices : 1
     Spare Devices : 2

            Layout : left-symmetric
        Chunk Size : 512K

Consistency Policy : bitmap

              Name : centos7:5  (local to host centos7)
              UUID : 38d5b8cf:6594c83a:b53ba5a4:2ecaffc3
            Events : 194

    Number   Major   Minor   RaidDevice State
       -       0        0        0      removed
       1       8       33        1      active sync   /dev/sdc1             
       -       0        0        2      removed

       0       8       17        -      faulty   /dev/sdb1                    ##########发现该盘坏了
       3       8       65        -      spare   /dev/sde1                      ##########之前的热备盘顶替了
       4       8       49        -      spare   /dev/sdd1


###第二次的适意情况
[root@centos7 ~]# mdadm --detail /dev/md5
/dev/md5:
           Version : 1.2
     Creation Time : Sun Mar 15 07:59:05 2020
        Raid Level : raid5
        Array Size : 377221120 (359.75 GiB 386.27 GB)
     Used Dev Size : 188610560 (179.87 GiB 193.14 GB)
      Raid Devices : 3
     Total Devices : 4
       Persistence : Superblock is persistent

     Intent Bitmap : Internal

       Update Time : Sun Mar 15 08:18:59 2020
             State : clean, degraded, recovering 
    Active Devices : 2
   Working Devices : 3
    Failed Devices : 1
     Spare Devices : 1

            Layout : left-symmetric
        Chunk Size : 512K

Consistency Policy : bitmap

    Rebuild Status : 0% complete                                        ###可以看见这是重构状态

              Name : centos7:5  (local to host centos7)
              UUID : 73e19978:73ff3239:db9abf74:c15a8f0e
            Events : 331

    Number   Major   Minor   RaidDevice State
       3       8       65        0      spare rebuilding   /dev/sde1           ####之前的热备盘正在重构
       1       8       33        1      active sync   /dev/sdc1
       4       8       49        2      active sync   /dev/sdd1

       0       8       17        -      faulty   /dev/sdb1




8、修复故障
先移除坏盘，若显示没反应可以su -刷新一下
[root@centos7 ~]# mdadm /dev/md5 --remove /dev/sdb1
mdadm: hot removed /dev/sdb1 from /dev/md5

[root@centos7 ~]# mdadm --detail /dev/md5 
Consistency Policy : bitmap

    Rebuild Status : 0% complete

              Name : centos7:5  (local to host centos7)
              UUID : 73e19978:73ff3239:db9abf74:c15a8f0e
            Events : 886                                                          ######注意这个在变化

    Number   Major   Minor   RaidDevice State
       3       8       65        0      spare rebuilding   /dev/sde1
       1       8       33        1      active sync   /dev/sdc1
       4       8       49        2      active sync   /dev/sdd1


#全0写修复磁盘
[root@centos7 ~]# mdadm --zero-superblock --force /dev/sdb1 
##将修复好的sdb1加入RADI组
[root@centos7 ~]# mdadm /dev/md5 --add /dev/sdb1 
mdadm: added /dev/sdb1

[root@centos7 ~]# mdadm --detail /dev/md5 
    Rebuild Status : 0% complete

              Name : centos7:5  (local to host centos7)
              UUID : 73e19978:73ff3239:db9abf74:c15a8f0e
            Events : 1362                                                          ######注意这个在变化

    Number   Major   Minor   RaidDevice State
       3       8       65        0      spare rebuilding   /dev/sde1
       1       8       33        1      active sync   /dev/sdc1
       4       8       49        2      active sync   /dev/sdd1

       5       8       17        -      spare   /dev/sdb1           ####加入到热备盘中，但是RAID5的重构真慢啊




9、使用
一般有lost+found/ 目录的表示是挂载盘成功。
[root@centos7 ~]# echo 'waht the fuck' > /mnt/raid5/test.conf
[root@centos7 ~]# cat /mnt/raid5/
lost+found/ test.conf   
[root@centos7 ~]# cat /mnt/raid5/test.conf 
waht the fuck

RAID6也是一样的操作等级改下就行。
































